# Suppress any remaining UserWarnings (just in case)
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Use only the community/up-to-date packages
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings

# 1Ô∏è‚É£ Load your textbook
print("üì• Loading textbook...")
loader = TextLoader("textbook.txt", encoding="utf-8")
docs = loader.load()
print(f"‚úÖ Loaded {len(docs)} document(s).")

# 2Ô∏è‚É£ Split into chunks
print("‚úÇÔ∏è Splitting documents into chunks...")
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = splitter.split_documents(docs)
print(f"‚úÖ Split into {len(texts)} chunks.")

# 3Ô∏è‚É£ Create embeddings
print("üî¢ Converting chunks into vector embeddings (this may take a while)...")
hf_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = Chroma.from_documents(texts, hf_model, persist_directory="./db")
print("‚úÖ Vector database created. Ready for questions!")

# 4Ô∏è‚É£ Initialize the updated Ollama LLM
llm = OllamaLLM(model="deepseek-r1:8b")

print("üìò Ready! Type your question (or 'exit' to quit).")

# 5Ô∏è‚É£ Streaming chat loop
while True:
    user_query = input("You: ")
    if user_query.lower() in ["exit", "quit"]:
        break

    # Retrieve relevant chunks
    print("üîç Searching for relevant textbook content...")
    docs = db.similarity_search(user_query)
    context = "\n".join([d.page_content for d in docs])

    # Prompt that allows reasoning
    prompt = f"""
Use the following textbook content as a reference. You may also reason using your general knowledge if needed, 
but prioritize the textbook content. Think step by step and provide clear, detailed explanations.

Textbook content:
{context}

Question: {user_query}
"""

    # Stream the response
    print("\nü§ñ DeepSeek: ", end="", flush=True)
    for chunk in llm.stream(prompt):
        print(chunk, end="", flush=True)
    print("\n")
